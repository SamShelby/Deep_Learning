{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780e61ad",
   "metadata": {},
   "source": [
    "## Aprendizaje Profundo: Tarea 1 - Ejercicio 2\n",
    "### Samuel López Ruiz\n",
    "---\n",
    "Programa el algoritmo de retropropagación usando NumPy para una tarea de clasificación binaria\n",
    "presuponiendo una red densa con dos capas ocultas y la función de pérdida de entropía cruzada\n",
    "binaria. Describe las fórmulas y reglas de actualización de los pesos y sesgos de cada capa y entrena\n",
    "y evalúa la red en algún conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c630ca",
   "metadata": {},
   "source": [
    "### Arquitectura \n",
    "La arquitectura de la red neuronal consiste de 2 capas ocultas con funciones de activación sigmoide. La siguiente figure presenta la arquitectura. \n",
    "<img src=\"img\\NN_np.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3dc89",
   "metadata": {},
   "source": [
    "### Nomenclatura\n",
    "- $g()$ funcion de activación de los nodos en capas ocultas\n",
    "- $g_o()$ función de activación de los nodos en la última capa\n",
    "- $k$ numero de capa\n",
    "- $j$ número de neurona\n",
    "- $i$ número de entrada\n",
    "\n",
    "La salida de una neurona está definida por \n",
    "$$o^k_j = g(a^k_j)$$\n",
    "### Pesos\n",
    "Para simplificar los cálculos y la programación, se considera que a los pesos $b$ como una entrada adicional con constante 1. Esto significa que la salida de suma-multiplicación se puede expresar asi:\n",
    "$$a_i^k=b_i^k+\\sum_{j=1}^{r_{k-1}} w^k_{ji}o^{k-1}_j=\\sum_{j=0}^{r_{k-1}} w^k_{ji}o^{k-1}_j$$\n",
    "donde $k$ es el número de la capa, $r_{k}$ son el número de neuronas por capa, $w_{ji}$ es el peso correspondiente a la neurona $j$ y a la entrada $i$ y $o^{k-1}_j$ es la salida después de la función de activación de la neurona $j$ y la capa $k-1$. <br>\n",
    "Notese como cambió la sumantoria de $j=1$ a $j=0$ por la incorporación del peso como una entrada. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9682e",
   "metadata": {},
   "source": [
    "### Gradiente del Error\n",
    "Para actualizar los pesos (el sesgo ya está incluido en los pesos), es necesario computar el gradiente del error con respecto a cada uno de los pesos. Usando la regla de la cadena, podemos expresar la derivada parcial como:\n",
    "$$\\frac{\\partial E}{\\partial w^k_{ij}} = \\frac{\\partial E}{\\partial a^k_{j}}\\frac{\\partial a^k_j}{\\partial w^k_{ij}}$$\n",
    "Sabemos que $$\\frac{\\partial a^k_j}{\\partial w^k_{ij}} = o^{k-1}_i$$\n",
    "y por convención el gradiente del error con respecto a la entrada se denomina:\n",
    "$$\\delta^k_j=\\frac{\\partial E}{\\partial a^k_{j}}$$\n",
    "\n",
    "Esto nos lleva a la expresión:\n",
    "$$\\frac{\\partial E}{\\partial w^k_{ij}} = \\delta^k_j o^{k-1}_i $$\n",
    "\n",
    "Para la última capa tenemos que el error con respecto a la entrada se puede definir como:\n",
    "$$\\delta^m_1 = g'_0(a_1^m)(\\hat{y}-y)$$\n",
    "donde $g'_0(a_1^m)$ es la derivada de la función de activación de la última capa evaluada en $a^m_1$ y $m$ es la úlitma capa\n",
    "\n",
    "### Gradiente capas intermedias\n",
    "Para las capas intermedias $(1<k<m)$   el error se puede escribir como:\n",
    "$$\\delta^k_j = \\frac{\\partial E}{\\partial a^k_{j}} = \\sum^{r^{k+1}}_{l=1} \\frac{\\partial E}{\\partial a^{k+1}_{l}}\\frac{\\partial a^{k+1}_j}{\\partial a^k_{j}}$$ \n",
    "Como podemos ver, se toma en cuenta el error de la capa siguiente $k+1$. \n",
    "Saltandonos varios pasos se llega a la siguiente expresión:\n",
    "$$\\delta^k_j = g'(a^k_j) \\sum^{r^{k+1}}_{l=1}w^{k+1}_{jl}\\delta^{k+1}_l$$ \n",
    "donde el error de la neurona $j$ en la capa $k$ está en función de la derivada de la función de activación de la neurona evaluada en $a^k_j$ y la suma-producto de los pesos de la siguiente capa y el error de la siguiente capa. <br>\n",
    "De aquí que viene el termino 'retro-propagación' porque para calcular el error de la capa $k$ se tiene que calcular primero el error de la capa $k+1$, lo que significa que el cómputo se realiza del final al inicio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb620b36",
   "metadata": {},
   "source": [
    "### Selección conveniente de $g()$\n",
    "Sabemos que la derivada de la función de activación sigmoide es:\n",
    "$$g'(x) = \\frac{\\partial \\sigma (x)}{\\partial x} = \\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "Y sabemos que:\n",
    "$$o^k_j = g(a^k_j) = \\sigma(a^k_j)$$\n",
    "Esto significa que la ecuación del error para la capas ocultas se puede simplificar de la siguiente forma:\n",
    "$$\\delta^k_j = o^k_j(1-o^k_j) \\sum^{r^{k+1}}_{l=1}w^{k+1}_{jl}\\delta^{k+1}_l$$ \n",
    "\n",
    "Con esta ecuación queda claro que se necesita primero calcular la salida de la capa $o^k_j$ en el 'forward-pass' y también se necesita el error de la siguiente capa $\\delta^{k+1}_l$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c01ee5",
   "metadata": {},
   "source": [
    "### Implementación\n",
    "Primero definimos la función sigmoide, de entropia curzada binaria y de exactitud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e895d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropia_cruzada_binaria(y, p):\n",
    "    p[p == 0] = np.nextafter(0., 1.)\n",
    "    p[p == 1] = np.nextafter(1., 0.)\n",
    "    return -(np.log(p[y == 1]).sum() + np.log(1 - p[y == 0]).sum())\n",
    "\n",
    "# define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def exactitud(y, y_predicha):\n",
    "    return (y == y_predicha).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0740a4",
   "metadata": {},
   "source": [
    "A continuación definimos el ciclo de retro-propagación considerando un tamaño de lote. Para calcular el gradiente  utilizando un lote de ejemplos, basta con computar el promedio del gradiente a lo largo de los ejemplos. Es decir:\n",
    "$$\\frac{\\partial E(X,\\Theta)}{\\partial w^k_{ij}} = \\frac{1}{N}\\sum^{N}_{d=1} \\frac{\\partial E_d}{\\partial w^{k}_{ij}}$$ \n",
    "Donde N es el tamaño del lote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050cdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backpropagation(X, y, num_hidden, batch_size, n_epochs=500, alpha=1):\n",
    "    n_batch = X.shape[0] // batch_size\n",
    "    y_hat = []\n",
    "    \n",
    "    # inicialización de los pesos W1, W2 y W_out dentro del rango (-1, 1)\n",
    "    W1 = 2 * np.random.random((X.shape[1] + 1, num_hidden)) - 1\n",
    "    W2 = 2 * np.random.random((num_hidden + 1, num_hidden)) - 1\n",
    "    W_out = 2 * np.random.random((num_hidden + 1, 1)) - 1\n",
    "      \n",
    "    # variables para guardar la pérdida y la exactitud a lo largo de las epocas\n",
    "    losses = np.zeros((n_epochs))\n",
    "    precisions = np.zeros((n_epochs))\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        for b in range(n_batch):\n",
    "            # tomamos el lote de ejemplos x con su etiqueta y\n",
    "            B = X[b*batch_size:b*batch_size+batch_size, :]\n",
    "            y_batch = y[b*batch_size:b*batch_size+batch_size, :]\n",
    "            \n",
    "            # 'forward-pass' para calcular las salidas de cada capa\n",
    "            # se agrega una columna con entrada 1 para simular el sesgo \n",
    "            a_input = np.hstack((np.ones((B.shape[0], 1)), B))\n",
    "            a_1 = np.hstack((np.ones((B.shape[0], 1)), sigmoid(np.dot(a_input, W1))))\n",
    "            a_2 = np.hstack((np.ones((B.shape[0], 1)), sigmoid(np.dot(a_1, W2))))\n",
    "            a_output = sigmoid(np.dot(a_2, W_out))\n",
    "            y_hat = a_output if b == 0 else np.vstack((y_hat, a_output))\n",
    "            \n",
    "            # fase de retro-propagación \n",
    "            # el gradiente del error con respecto a la última función de activación\n",
    "            output_error = a_output - y_batch\n",
    "            \n",
    "            # el error para cada una de las capas intermedias\n",
    "            # se empieza con la última capa hasta llegar a la primera\n",
    "            error_2 = a_2[:, 1:] * (1 - a_2[:, 1:]) * np.dot(output_error, W_out.T[:, 1:])\n",
    "            error_1 = a_1[:, 1:] * (1 - a_1[:, 1:]) * np.dot(error_2, W2.T[:, 1:])\n",
    "            \n",
    "        \n",
    "            # cálculo de derivadas parciales a lo largo de todos los ejemplos del lote\n",
    "            hidden_pd_1 = a_input[:, :, np.newaxis] * error_1[: , np.newaxis, :]\n",
    "            hidden_pd_2 = a_1[:, :, np.newaxis] * error_2[: , np.newaxis, :]\n",
    "            output_pd = a_2[:, :, np.newaxis] * output_error[:, np.newaxis, :]\n",
    "        \n",
    "            # se calcula el promedio del gardiente a lo largo de los ejemplo \n",
    "            total_gradient_1 = np.average(hidden_pd_1, axis=0)\n",
    "            total_gradient_2 = np.average(hidden_pd_2, axis=0)\n",
    "            total_out_gradient = np.average(output_pd, axis=0)\n",
    "        \n",
    "            # se actualizan los pesos\n",
    "            W1 += - alpha * total_gradient_1\n",
    "            W2 += - alpha * total_gradient_2\n",
    "            W_out += - alpha * total_out_gradient\n",
    "            \n",
    "            \n",
    "        # calcula y guarda la pérdida y exactitud en la época \n",
    "        losses[i] = entropia_cruzada_binaria(y, y_hat)\n",
    "        precisions[i] = exactitud(y, np.round(y_hat))\n",
    "\n",
    "    return losses, precisions, y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65644e89",
   "metadata": {},
   "source": [
    "Usamos un set de datos inventados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd61cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida Después de Entrenamiento: \n",
      "[[0.  ]\n",
      " [0.99]\n",
      " [0.99]\n",
      " [0.01]]\n",
      "Precision: \n",
      "100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# semilla para garantizar replicabilidad\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "# Usamos los datos de la compuerta (XOR)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "# utilizamos 500 epocas, 4 neuronas por capa y un lote de 2\n",
    "losses, precisions, y_hat = backpropagation(X, y, num_hidden=5, n_epochs=400, batch_size=2, alpha=1)\n",
    "\n",
    "print(\"Salida Después de Entrenamiento: \\n{}\".format(np.round(y_hat, 2)))\n",
    "print(\"Precision: \\n{}\".format(precisions[-1]))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(losses.size), losses, label='ECB')\n",
    "plt.plot(np.arange(precisions.size), precisions, label='Exactitud')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801c017",
   "metadata": {},
   "source": [
    "Se puede comprobar que la red neuronal fue capaz de aprender la compuerta XOR. El error fue disminuyendo a lo largo de las epocas y la exactitud llegó a 100% antes de la epoca 200."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-a2cd6d97",
   "language": "python",
   "display_name": "PyCharm (OptimizationAlgorithms)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}